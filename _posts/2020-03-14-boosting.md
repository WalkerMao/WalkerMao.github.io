---
layout: post
title:  "Boosting Methods"
date: 2020-03-14
categories: ml
comments: true
---

In Boosting, we construct a strong predictive algorithm by iteratively layering weak predictive algorithms (weak learners). 

A weak learner is one whose error rate is only slightly better than random guessing. The purpose of boosting is to sequentially apply the weak predictive algorithm to repeatedly modified versions of the data, thereby producing a sequence of weak learners: $G_m(x), m = 1, 2, \cdots, M$.

The predictions from all of them are then combined through a weighted majority vote to produce the final prediction:

$$
G(x) = \text{sign} \Big(\sum_{m=1}^{m} α_m G_m(x) \Big).
$$

Here $α_1,α_2,\cdots ,α_M$ are computed by the boosting algorithm, and weight the contribution of each respective $G_m(x)$. Their effect is to give higher influence to the more accurate learners in the sequence. The data modifications at each boosting step consist of applying weights $w_1,w_2,\cdots, w_N$ to each of the training observations $$(x_i,y_i),i=1,\cdots,N$$.

## AdaBoost

AdaBoost, short for Adaptive Boosting, is a basic boosting methods. It can be implemented for both regression and classification problems. 

<img src="https://static.packt-cdn.com/products/9781789953633/graphics/23c1ad86-604f-439f-9597-73c645e41a74.png" alt="AdaBoost" style="zoom: 70%;" />

The algorithm below is the AdaBoost.M1 algorithm that used for classification problem. 

**Algorithm. AdaBoost.M1.**

[1] Initialize the observation weights $w_i = \frac{1}{N}, i=1,2,\cdots,N$.

[2] For $m = 1$ to $M$, execute the steps (a) to (d):

 (a) Fit a classifier $G_m(x)$ to the training data using weights $w_i$;

 (b) Compute $$\text{err}_m = \frac{\sum_{i=1}^{N} w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^{N} w_i}$$;

 (c) Compute $$ α_m = \log \big( \frac{1 − \text{err}_m}{\text{err}_m} \big)$$;

 (d) Set $$ w_i ← w_i · \exp[α_m · I(y_i \neq G_m (x_i))] $$.

[3] Output $G(x) = \text{sign} \Big[\sum_{m=1}^{M} α_M G_m(x) \Big].$

## Boosting Fits an Additive Model

Boosting is a way of fitting an additive expansion in a set of elementary "basis" functions. The philosophy is like Taylor expansion. 

Basis function expansions take the form

$$
f(x) = \sum_{m=1}^{M} \beta_m b(x; \gamma_m),
$$

where $β_m , m = 1, 2, . . . , M$ are the expansion coefficients, and $b(x; γ) ∈ \mathbb{R}$ are usually simple basis functions of the multivariate argument $x$, characterized by a set of parameters $γ$. 

In the algorithm AdaBoost.M1. The basis functions are the individual classifiers $$G_m(x) \in \{−1, 1\}$$.

**Algorithm: Forward Stagewise Additive Modeling.** 

[1] Initialize $f_0(x) = 0$.

[2] For $m = 1$ to $M$:

 (a) Compute $$ (β_m, γ_m) = \operatorname*{argmin}_{β,γ} \sum_{i=1}^{N} L\big(y_i, f_{m−1}(x_i) + βb(x_i;γ)\big)$$.

 (b) Set $f_m(x) = f_{m−1}(x) + β_mb(x;γ_m)$.

[3] Output $f_M(x)$.

Suppose $\gamma$ is of dimension $p_{\gamma}$. In $m$-th step, we need to do optimization with $1+p_{\gamma}$ parameters ($β_m$ is $1$ dimension and $γ_m$ is $p_{\gamma}$). At the end, $f_M(x)$ have $M(1+p_{\gamma})$ parameters. That is much easier than we do optimization with $M(1+p_{\gamma})$ parameters: $$(β, γ) = \operatorname*{argmin}_{β,γ} \sum_{i=1}^{N} L\big(y_i, f_M(x_i)\big)$$. 

AdaBoost.M1 is equivalent to forward stagewise additive modeling using the exponential loss function $L(y, f (x)) = \exp(−y f (x))$. Exponential loss is suitable for classification but not for regression.

**Gradient boosting and XGBoost use the gradient descent and Newton's method respectively to do the step [2] (a) in the algorithm above, and minimize the loss (or objective) function iteratively.** 

## Boosting Trees

Tree ensemble methods (boosting: e.g. GBM, XGBoost, bagging: e.g. random forest) are widely used. Almost half of data mining competition are won by using some variants of tree ensemble methods. 

As for CART, a constant $γ_j$ is assigned to each region $R_j$ and the predictive rule is $x ∈ R_j ⇒ f(x)=γ_j$. 

Thus a tree can be expressed as the additive model form:

$$
T(x;Θ) = \sum_{j=1}^{J} γ_j I(x∈R_j),
$$

with parameters $$Θ = \{R_j , γ_j\}^J_1$$. $$J$$ is usually treated as a meta-parameter. 

The parameters are found by minimizing the training loss 

$$
\hat{Θ} = \operatorname*{argmin}_{Θ} \sum_{j=1}^{J} \sum_{x_i \in R_j} L(y_i,γ_j),
$$

It is useful to divide the optimization problem into two parts:

1. Finding $γ_j$ given $R_j$: Given the $R_j$, often $\hat{γ}_j = \bar{y}_j$ , the mean of the $y_i$ falling in region $R_j$.

2. Finding $R_j$: A typical strategy is to use a greedy, top-down recursive partitioning algorithm to find the $R_j$. 

The **boosted tree** model is a sum of such trees, 

$$
f_M(x) = \sum_{m=1}^{M} T(x; Θ_m),
$$

induced in a forward stagewise manner (Algorithm: Forward Stagewise Additive Modeling). At each step in the forward stagewise procedure one must solve 

$$
\hat{Θ}_m = \operatorname*{argmin}_{Θ_m} \sum_{i=1}^{N} L \big(y_i, f_{m-1}(x_i) + T(x_i;\Theta_m) \big)
$$

for $$ \Theta_m = \{ R_{jm}, \gamma_{jm} \}_{j=1}^{J_m} $$ of the next tree, given the current model $f_{m-1}(x)$. 

For $m$-th tree, given the regions $R_{jm}$'s ($j=1,\cdots,J_m$), finding the optimal constants $γ_{jm}$'s in each region is straightforward and easy. However, finding the regions $R_{jm}$'s for the $m$-th tree is difficult. For squared-error loss, a special case, $$\hat{\Theta}_m$$ is simply the parameters of the regression tree that best predicts the current residuals $$y_i - f_{m-1}(x_i), (i=1, \cdots,N)$$.

## Gradient Boosting

Gradient boosting is based on gradient descent. Instead of minimizing, gradient boosting use the gradient descent to do the step [2] (a) in the algorithm Forward Stagewise Additive Modeling.

We induce a weak regressor or classifier $h_m(x)$ (e.g. a tree $T(x;Θ_m)$) at the $m$-th iteration to fit the negative gradient, which means the predictions (a vector with dimension $n \times 1$) by weak model $h_m(x)$ are as close as possible to the negative gradient. 

We want to minimize $L(y_i, f(x_i)), i=1,...N$, where $f$ is the independent variable. 

Recall Taylor expansion: $f(x+\Delta x) \approx f(x)+f'(x)\Delta x$. at $m$-th step, we want to minimize the loss function 

$$
\sum_{i=1}^{N} L\Big(y_i,f_{m-1}(x_i)+h_m(x_i)\Big) \approx \sum_{i=1}^{N} \Big[L(y_i, f_{m-1}(x_i)) + g_{im} h_m(x_i) \Big].
$$

As shown in my [earlier blog](https://walkermao.github.io/intro-to-ml.html), $$h_m(x_i)$$ should be $- \eta_m g_{im}$, where $\eta_m$ is the step size and  $$ g_{im} = \frac{∂L(y_i,f_{m-1}(x_i))}{∂f_{m-1}(x_i)} $$ is the gradient. 

Thus, we have the iterative formula: 

$$
f_m(x_i) = f_{m-1}(x_i) - \eta_m g_{im}.
$$

If $L(y_i, f_{m-1}(x_i))=\frac{1}{2}[y_i-f_{m-1}(x_i)]^2$, then the gradient $\frac{∂L(y_i,f (x_i))}{∂f_{m-1}(x_i)}=f_{m-1}(x_i)-y_i$, which is the residual $r_{im}$. 

**Algorithm: Gradient Tree Boosting Algorithm.**

[1] Initialize $$f_0(x) = \operatorname*{argmin}_γ \sum_{i=1}^N L(y_i,γ)$$.

[2] For $m = 1$ to $M$, execute (a) to (e):

 (a) For $i = 1,2,...,N$ compute

$$
g_{im} = \bigg[ \frac{∂L(y_i,f (x_i))}{∂f(x_i)} \bigg] _{f=f_{m-1}}.
$$

 (b) Fit a regression tree to the targets $-g_{im}$'s to get terminal regions $R_{jm},j=1,2,..,J_m$.

 (c) For $j = 1,2,...,J_m$, compute $γ_{jm}$. Now we get a new tree 

$$
h_m(x) = T(x; Θ_m) = \sum_{j=1}^{J_m} γ_{jm}I(x \in R_{jm}).
$$

 (d) Compute step size $$ \eta_m = \operatorname*{argmin}_{\eta} \sum_{i=1}^N L\big(y_i,f_{m-1}(x_i)+ \eta h_m(x_i) \big) $$.

 (e) Update 

$$
f_m(x) = f_{m−1}(x) + \eta_m h_m(x),
$$

 where $h_m(x) = \sum_{j=1}^{J_m} γ_{jm}I(x \in R_{jm})$ is a new tree ($m$-th tree).

[3] Output $\hat{f}(x) = f_M(x)$.

## XGBoost

XGBoost add a regularization term to construct the objective function. At $m$-th step, we want to minimize the objective 

$$
\text{Obj}^{(m)} = \sum_{i=1}^{N}L(y_i, f_{m-1}(x)+h_m(x_i)) + \Omega(f_{m-1}+h_{m}),
$$

then 

$$
h_m = \operatorname*{argmin}_{h_m} \text{Obj}^{(m)}.
$$

In the formula above, $\Omega(f_m)$ is the **regularization** term, which is used to measure the complexity of $f_m$. 

$$
\Omega(f_m) = \mu T + \frac{1}{2}\lambda \sum_{j=1}^{T} w_j^2,
$$

where $T$ is the number of leaves, $w_j$ is the [prediction score](https://stats.stackexchange.com/questions/353462/what-are-the-implications-of-scaling-the-features-to-xgboost) of leaf $j$, and $\sum_{j=1}^{T} w_j^2$ is the L2 norm of leaf scores. 

[Why penalize $\sum_{j=1}^{T} w_j^2$ to control the complexity?](https://stats.stackexchange.com/questions/178012/definition-of-complexity-of-a-tree-in-xgboost) In this case, a large value of $w_j$ would correspond to a terminal (leaf) node giving a very large and significant update to the prior model. However, the idea of a gradient booster is to carefully and slowly reduce the bias of the model by adding these trees one by one.

XGBoost use Newthon's method to minimize the objective function iteratively. As shown in my [earlier blog](https://walkermao.github.io/intro-to-ml.html), we use Taylor expansion of second order to get the approximation of objective function. Recall Taylor expansion: $$f(x+\Delta x) \approx f(x)+f'(x)\Delta x + \frac{1}{2} f''(x)\Delta x^2$$. We do that for $$L(y_i, f_{m-1}(x)+h_m(x_i))$$, and we have

$$\begin{align*}
\text{Obj}^{(m)} &= \sum_{i=1}^{N} L\big(y_i, f_{m-1}(x)+h_m(x_i)\big) + \Omega(f_{m-1}) + \Omega(h_{m})\\ 
&\approx \sum_{i=1}^{N} \big[L(y_i, f_{m-1}(x)) + g_{im}h_m(x_i) + \frac{1}{2}h_{im}h_m^2(x_i)\big] + \Omega(f_{m-1}) + \Omega(h_{m}),
\end{align*}$$

where $g_{im} = \frac{∂L(y_i,f_{m-1}(x_i))}{∂f_{m-1}(x_i)}, h_{im} = \frac{∂^2L(y_i,f_{m-1}(x_i))}{∂f_{m-1}^2(x_i)}$. 

Here is a brief description of **XGBoost algorithm** (read references for computation details): 

[1] For $m = 1,...,M$, execute (a) and (b):

(a) $$\begin{align*}
   h_m = \operatorname*{argmin}_{h_m} \text{Obj}^{(m)} = \operatorname*{argmin}_{h_m} \sum_{i=1}^{N} \big[g_{im}h_m(x_i) + \frac{1}{2}h_{im}h_m^2(x_i)\big] + \Omega(h_{m}).
   \end{align*}$$

(b) $$f_m(x) = f_{m−1}(x) + \eta_m h_m(x)$$ where $\eta_m$ is called step-size or learning rate, usually set around $0.1$. 

[2] Output $\hat{f}(x) = f_M(x)$.

The step-size or learning rate $\eta_m$ is mostly smaller than $1$, usually set around $0.1$, that means we do not do full optimization in each step and reserve chance for future rounds, it helps prevent overfitting. 

**Regularization and Taylor expansion of second order are the key reasons why XGBoost is better than GBM.** 

---

**References:**

Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. *The elements of statistical learning*. Vol. 1. No. 10. New York: Springer series in statistics, 2001.

"Introduction to Boosted Trees." *Introduction to Boosted Trees - Xgboost 1.1.0-SNAPSHOT Documentation*, xgboost.readthedocs.io/en/latest/tutorials/model.html.

张 凌寒. "GBDT与XGBoost解析及应用." *腾讯云*, 13 May 2019, cloud.tencent.com/developer/article/1424251.
