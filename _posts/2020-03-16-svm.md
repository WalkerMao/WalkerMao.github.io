---
layout: post
title:  "Support Vector Machines (SVMs)"
date: 2020-03-16
categories: ml
comments: true
---

## Intro to SVM

Suppose $$y_i \in \{-1,1\}$$. Denote the two hyperplanes as $$H_1 = \{x \in \mathbb{R}^p : w^Tx + b = 1\}$$ and $$H_2 = \{x \in \mathbb{R}^p : w^Tx + b = -1\}$$ for some $$w \in \mathbb{R}^p, b \in \mathbb{R}$$. 

We want to make these two hyperplanes separate the data points, and also maximize the margin between the hyperplanes. The margin between $$H_1$$ and $$H_2$$ is $$\frac{2}{\| w \|_2}$$. Note that maximizing the margin $$\frac{2}{\| w \|_2}$$ is the same as minimizing $$\frac{\|w\|_2}{2}$$.

For linearly separable data, SVM (support vector machine) finds maximum-marginal linear separator:

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} \\
& \text{ subject to } y_i(w^Tx_i+b) \geq 1, \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

The points $$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) = 1\} $$ are the support vectors.

Real data is usually not linearly separable. We use the hinge loss to measure misclassification error:

$$
\ell(y_i,z_i)=\max(0,1-y_i \cdot z_i).
$$

Here we set $$z_i=w^Tx_i+b$$, then we get what we call **slack variables** $$\xi_i$$ to account for violations for linear separability, as

$$
\xi_i = \max\left(0, 1 - y_i(w^Tx_i+b)\right).
$$

The plot of $$\xi_i$$ is as shown below.

<div align="center"> <img src="../pictures/svm-hinge-loss.png" alt="svm-hinge-loss" style="zoom:50%;" /> </div>

For a hyperparameter $$C > 0$$, the **soft margin SVM** is then:

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i\\
& \text{ subject to } 
\begin{aligned} 
& y_i(w^Tx_i+b) \geq 1-\xi_i,  \\ & \xi_i\geq0,\forall i=1,\cdots,n. \end{aligned}
\end{aligned}
\end{equation*}
$$

The points $$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) \leq 1\} $$ are the support vectors.

Let $$\hat{w}, \hat{b}$$ be the solution of the optimization, then the **classifier** is $$\hat{f}(x) = \text{sign}(\hat{w}^Tx + \hat{b})$$.

<br>

<div align="center"> <img src="../pictures/svm_slack.png" alt="svm_slack" style="zoom:100%;" /> </div>

<br>

## Lagrange Duality

The Lagrange function of the optimization problem of soft margin SVM is 

$$
\mathcal{L}(w,b,\xi,\alpha,\mu) = \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i + \sum_{i=1}^n \alpha_i [1 - \xi_i - y_i(w^Tx_i + b)] - \sum_{i=1}^n\mu_i\xi_i,
$$

where $$\alpha_i \geq 0, \mu_i \geq 0$$ are the Lagrange multipliers.

Take derivative of the Lagrange function with respect to $$w,b,\xi_i$$ and set equal to $$0$$, and we have

$$
\begin{aligned}
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial w} = w - \sum_{i=1}^{n} \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^{n} \alpha_i y_i x_i, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial b} = - \sum_{i=1}^{n} \alpha_i y_i = 0 \implies \sum_{i=1}^{n} \alpha_i y_i = 0, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies C = \alpha_i + \mu_i.
\end{aligned}
$$

Plug these three equations into the optimization formulation of soft margin SVM, then we get the dual optimization problem:

$$
\begin{equation*}
\begin{aligned}
& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}, \\
& \text{ subject to } 
\begin{cases} 
\sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C ,\forall i=1,\cdots,n. 
\end{cases}
\end{aligned}
\end{equation*}
$$

Note that there are inequity constraints in the optimization formulation, which produce the KKT condtions:

$$
\begin{cases}
\alpha_i \geq 0, \\
\mu_i \geq 0, \\
\alpha_i[y_i(w^Tx_i+b) - 1 + \xi_i] = 0, \\
\xi_i \geq 0, \\
\mu_i\xi_i = 0.
\end{cases}
$$

We use the **SMO** (sequential minimal optimization) algorithm to solve the dual optimization problem. It simply does the following:

Repeat till convergence {

1. Select some pair $$α_i$$ and $$α_{i'}$$ to update next (using a heuristic that tries to pick the two that will allow us to make the biggest progress towards the global maximum).

2. Reoptimize $$\mathcal{D}(\alpha)$$ with respect to $$α_i$$ and $$α_{i'}$$ , while holding all the other $$α_{i''}$$'s $$(i'' \neq i, i')$$ fixed.
   }

After getting the solution of the dual optimization problem $$\hat{\alpha}_i, i=1,\cdots,n$$, we have the weight

$$
\hat{w} = \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}.
$$

The support vectors are the points $$(x_i,y_i)$$ that have the indexes in the set:

$$
\mathbb{I}^* = \{ i^*: 0<\hat{\alpha}_{i^*}<C \}.
$$

Then we compute the intercept $$\hat{b}$$ by the support vectors: 

$$
\hat{b} = \frac{1}{\mid\mathbb{I^*}\mid} \sum_{i \in \mathbb{I^*}} ( y_{i^*} - \hat{w}^Tx_{i^*} )
$$

The classifier is $$\hat{f}(x) = \text{sign}(\hat{w}^Tx + \hat{b})$$. 

## Kernel SVM

If the data is not linearly separable and it has finite features, mapping to a higher dimensional feature space makes it linearly separable. 

<div align="center"> <img src="../pictures/Kernel-SVM.png" alt="Kernel-SVM" style="zoom: 60%;" /> </div>

Denote $$\phi(x)$$ as the higher dimensional feature vector after mapping $$x$$ to a higher dimensional feature space. Then the hyperplanes will be $$\{x \in \mathbb{R}^p : w^T \phi(x) + b = \pm 1\}$$. 

The soft margin SVM is then: 

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i, \\
& \text{ subject to } 
\begin{cases} 
 y_i(w^T \phi(x_i)+b) \geq 1-\xi_i,  \\ 
 \xi_i\geq0,\forall i=1,\cdots,n. 
\end{cases}
\end{aligned}
\end{equation*}
$$

The dual optimization problem:

$$
\begin{equation*}\begin{aligned}& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \phi(x_i)^T \phi(x_{i'}), \\
& \text{ subject to } \begin{cases} \sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C ,\forall i=1,\cdots,n. \end{cases}\end{aligned}\end{equation*}
$$

Since the feature vector $$\phi(x)$$ may have high dimension, the computation of the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ can be expensive. To deal with that, we use the kernel trick. 

A **kernel function** is defined as
$$
\mathcal{K}(x_i, x_{i'}) =  \phi(x_i)^T \phi(x_{i'}).
$$

With the kernel function, we can compute the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ by using $$x_i, x_{i'}$$ that are in the **original space**, and that is much computationally cheaper than computing $$ \phi(x_i)^T \phi(x_{i'}) $$ directly.

The **kernel matrix** is defined as 

$$
\mathbf{K} = 
\begin{pmatrix}
  \mathcal{K}(x_1, x_1) & \mathcal{K}(x_1, x_2) & \cdots & \mathcal{K}(x_1, x_{n}) \\
\mathcal{K}(x_2, x_1) & \mathcal{K}(x_2, x_2) & \cdots & \mathcal{K}(x_2, x_{n}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \mathcal{K}(x_n, x_1) & \mathcal{K}(x_n, x_2) & \cdots & \mathcal{K}(x_n, x_{n})
 \end{pmatrix}.
$$

Some commonly used kernel functions:

| Name              | Formula                                                      | Hyperparameter                |
| ----------------- | ------------------------------------------------------------ | ----------------------------- |
| Linear            | $$\mathcal{K}(x_i, x_{i'})=x_i^Tx_{i'}$$                     |                               |
| Polynomial        | $$\mathcal{K}(x_i, x_{i'})=(\beta x_i^Tx_{i'}+\theta)^d$$    | polynomal degree $$d \geq 1$$ |
| RBF (or Gaussian) | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'} \|/2\sigma^2)$$ | width $$\sigma>0$$            |
| Laplace           | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'}\|/\sigma)$$ | $$\sigma>0$$                  |
| Sigmoid           | $$\mathcal{K}(x_i, x_{i'})=\text{tanh}(\beta x_i^Tx_{i'} + \theta)$$ | $$\beta>0, \theta<0$$         |

How to choose?

1. If the number of features is large, and sample size is small, the data is usually linearly separable, then use linear kernel or logistic regression. 
2. If the number of features is small, and sample size is normal, then use RBF kernel.
3. If the number of features is small, and sample size is large, then add some features and it becomes the first case.

Note that SVMs with linear kernel are linear classifiers, and SVMs with other kernels are non-linear classifiers. 

## SVR

In SVR (support vector regression), we tolerate an error $$\epsilon$$ between $$w^Tx+b$$ and $$y$$, which means the error will not be considered if it is within $-\epsilon$ and $$\epsilon$$. Note that $$\epsilon$$ is a hyperparameter that we should set before training the model.

<div align="center"> <img src="../pictures/SVR_2.png" alt="SVR_2" style="zoom:100%;" /> </div>

## Tips

SVM is good for small sample size, compared to other machine learning methods, since we only need a few support vectors to determine the hyperplanes. SVM is also suitable for non-linear and high dimensional problems.  

The local optimum of SVM must be the global optimum, since it is a convex optimization problem.

<br>

**References**: 

周志华. *机器学习*. 清华大学出版社, 2016. 

张皓. [从零推导支持向量机（SVM）](https://link.zhihu.com/?target=https%3A//github.com/HaoMood/File/raw/master/%25E4%25BB%258E%25E9%259B%25B6%25E6%259E%2584%25E5%25BB%25BA%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA%28SVM%29.pdf).
