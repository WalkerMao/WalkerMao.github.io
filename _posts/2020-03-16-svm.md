---
layout: post
title:  "Support Vector Machines (SVMs)"
date: 2020-03-16
categories: ML
comments: true
---

## Intro to SVM

Suppose $$y_i \in \{-1,1\}$$. Denote the two hyperplanes for some $$w \in \mathbb{R}^p, b \in \mathbb{R}$$ as

$$
\begin{aligned} & H_1 = \{x \in \mathbb{R}^p : w^Tx + b = 1\}, \\ & H_2 = \{x \in \mathbb{R}^p : w^Tx + b = -1\}. \end{aligned}
$$

We want to make these two hyperplanes separate the data points, and also maximize the margin between the hyperplanes. The margin between $$H_1$$ and $$H_2$$ is $$\frac{2}{\| w \|_2}$$. Note that maximizing the margin $$\frac{2}{\| w \|_2}$$ is the same as minimizing $$\frac{\|w\|_2}{2}$$.

For linearly separable data, SVM (support vector machine) finds maximum-marginal linear separator:

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2}, \\
& \text{ subject to } y_i(w^Tx_i+b) \geq 1, \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

The points $$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) = 1\} $$ are the support vectors.

## Soft Margin SVM

Real data is usually not linearly separable. Here we set 

$$
z_i=y_i(w^Tx_i+b)-1,
$$

then the misclassified data points are $$\{(x_i, y_i): z_i<0\} $$. 

We can use loss functions to measure the misclassification error. Such as 0/1 loss:

$$
\ell_{\text{0/1}}(z) = \mathbf{1}(z<0).
$$

However, this loss function is not convex and also not continuous, thus it is not amenable to numerical optimization. For this reason, we can use some convex and continuous loss functions called **surrogate loss**. Here are three commonly used surrogate loss functions:

$$
\begin{aligned}
& \text{Hinge loss: } & \ell_{\text{hinge}}(z) = \max(0, 1  - z); \\
& \text{Expomemetoal loss: } & \ell_{\text{exp}}(z) = e^{-z}; \\
& \text{Logistic loss: } & \ell_{\text{log}}(z) = \ln(1 + e^{-z}).
\end{aligned}
$$

Here is the plot for these four loss functions: 

<div align="center"> <img src="../pictures/surrogate-loss-functions.png" alt="surrogate-loss-functions.png" style="zoom:38%;" /> </div>

In the following, we use hinge loss as the surrogate loss. 

We introduce **slack variables** $$\xi_i$$ to account for violations for linear separability, as

$$
\xi_i = \ell_{\text{hinge}}(z_i+1) = \max\left(0, 1 - y_i(w^Tx_i+b)\right).
$$

The plot of $$\xi_i$$ is as shown below.

<div align="center"> <img src="../pictures/svm-hinge-loss.png" alt="svm-hinge-loss" style="zoom:50%;" /> </div>

For a hyperparameter $$C > 0$$, the **soft margin SVM** is then:

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i,\\
& \text{ subject to } 
\begin{cases} 
y_i(w^Tx_i+b) \geq 1-\xi_i,  \\ \xi_i\geq0, 
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

The points $$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) \leq 1\} $$ are the support vectors.

Let $$\hat{w}, \hat{b}$$ be the solution of the optimization, then the **classifier** is $$\hat{f}(x) = \text{sign}(\hat{w}^Tx + \hat{b})$$.

<br>

<div align="center"> <img src="../pictures/svm_slack.png" alt="svm_slack" style="zoom:100%;" /> </div>

<br>

## Lagrange Duality

The Lagrange function of the optimization problem of soft margin SVM is 

$$
\mathcal{L}(w,b,\xi,\alpha,\mu) = \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i + \sum_{i=1}^n \alpha_i [1 - \xi_i - y_i(w^Tx_i + b)] - \sum_{i=1}^n\mu_i\xi_i,
$$

where $$\alpha_i \geq 0, \mu_i \geq 0$$ are the Lagrange multipliers.

Take derivative of the Lagrange function with respect to $$w,b,\xi_i$$ and set equal to $$0$$, and we have

$$
\begin{aligned}
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial w} = w - \sum_{i=1}^{n} \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^{n} \alpha_i y_i x_i, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial b} = - \sum_{i=1}^{n} \alpha_i y_i = 0 \implies \sum_{i=1}^{n} \alpha_i y_i = 0, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies C = \alpha_i + \mu_i.
\end{aligned}
$$

Plug these three equations into the optimization formulation of soft margin SVM, then we get the dual optimization problem:

$$
\begin{equation*}
\begin{aligned}
& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}, \\
& \text{ subject to } 
\begin{cases} 
\sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C ,
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

Note that there are inequity constraints in the optimization formulation, which produce the KKT condtions:

$$
\begin{cases}
\alpha_i \geq 0, \\
\mu_i \geq 0, \\
\alpha_i[y_i(w^Tx_i+b) - 1 + \xi_i] = 0, \\
\xi_i \geq 0, \\
\mu_i\xi_i = 0.
\end{cases}
$$

We use the **SMO** (sequential minimal optimization) algorithm to solve the dual optimization problem. It simply does the following:

Repeat till convergence {

1. Select some pair $$α_i$$ and $$α_{i'}$$ to update next (using a heuristic that tries to pick the two that will allow us to make the biggest progress towards the global maximum).

2. Reoptimize $$\mathcal{D}(\alpha)$$ with respect to $$α_i$$ and $$α_{i'}$$ , while holding all the other $$α_{i''}$$'s $$(i'' \neq i, i')$$ fixed.
   }

After getting the solution of the dual optimization problem $$\hat{\alpha}_i, i=1,\cdots,n$$, we have the weight

$$
\hat{w} = \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}.
$$

The support vectors are the points $$(x_i,y_i)$$ that have the indexes in the set:

$$
I^* = \{ i^*: 0<\hat{\alpha}_{i^*}<C \}.
$$

Then we compute the intercept $$\hat{b}$$ by the support vectors: 

$$
\hat{b} = \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} ( y_{i^*} - \hat{w}^Tx_{i^*} )
$$

The output of the classifier is 

$$
\hat{y} = \text{sign}(\hat{w}^Tx + \hat{b}).
$$

## Kernel SVM

If the data is not linearly separable and it has finite features, mapping to a higher dimensional feature space makes it linearly separable. 

<div align="center"> <img src="../pictures/Kernel-SVM.png" alt="Kernel-SVM" style="zoom: 60%;" /> </div>

Denote $$\phi:\mathbb{R}^p \to \mathbb{R}^\tilde{p}$$ as a mapping function and $$\phi(x)$$ as the higher dimensional feature vector after mapping $$x$$ to a higher dimensional feature space:

$$
x \in \mathbb{R}^p \to \phi(x) \in \mathbb{R}^\tilde{p}.
$$

Then the hyperplanes will be 

$$
\{x \in \mathbb{R}^p : w^T \phi(x) + b = \pm 1\}.
$$

The soft margin SVM is then: 

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i, \\
& \text{ subject to } 
\begin{cases} 
 y_i(w^T \phi(x_i)+b) \geq 1-\xi_i,  \\ 
 \xi_i \geq 0, 
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

The dual optimization problem:

$$
\begin{equation*}\begin{aligned}& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \phi(x_i)^T \phi(x_{i'}), \\
& \text{ subject to } \begin{cases} \sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C , \end{cases} \ \ \forall i=1,\cdots,n. \end{aligned} \end{equation*}
$$

Since the feature vector $$\phi(x)$$ may have high dimension $$\tilde{p}$$, the computation of the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ is $$O(\tilde{p})$$ and is very expensive. To deal with that, we use the kernel trick. 

A **kernel function** is defined as

$$
\mathcal{K}(x_i, x_{i'}) =  \phi(x_i)^T \phi(x_{i'}).
$$

With the kernel function, we can compute the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ by using $$x_i, x_{i'}$$ that are in the **original space**, and the computation of $$\mathcal{K}(x_i, x_{i'})$$ is $$O(p)$$. By using kernel function, the computation is reduced from $$O(\tilde{p})$$ to $$O(p)$$. That means computing $$\mathcal{K}(x_i, x_{i'})$$ is much cheaper than computing $$ \phi(x_i)^T \phi(x_{i'}) $$ directly.

The **kernel matrix** is defined as 

$$
\mathbf{K} = 
\begin{pmatrix}
  \mathcal{K}(x_1, x_1) & \mathcal{K}(x_1, x_2) & \cdots & \mathcal{K}(x_1, x_{n}) \\
\mathcal{K}(x_2, x_1) & \mathcal{K}(x_2, x_2) & \cdots & \mathcal{K}(x_2, x_{n}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \mathcal{K}(x_n, x_1) & \mathcal{K}(x_n, x_2) & \cdots & \mathcal{K}(x_n, x_{n})
 \end{pmatrix}.
$$

Kernel matrix is always positive semi-definite. Actually a function $$\mathcal{K}(x_i, x_{i'})$$ can be used as a kernel function if and only if it is symmetric and the corresponding kernel matrix is positive semi-definite. The function is symmetric means that $$\mathcal{K}(x_i, x_{i'}) = \mathcal{K}(x_{i'}, x_i)$$ for all $$i,i'$$.

Some commonly used kernel functions:

| Name              | Formula                                                      | Hyperparameter                |
| ----------------- | ------------------------------------------------------------ | ----------------------------- |
| Linear            | $$\mathcal{K}(x_i, x_{i'})=x_i^Tx_{i'}$$                     |                               |
| Polynomial        | $$\mathcal{K}(x_i, x_{i'})=(x_i^Tx_{i'})^d$$                 | polynomal degree $$d \geq 1$$ |
| RBF (or Gaussian) | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'} \|/2\sigma^2)$$ | width $$\sigma>0$$            |
| Laplace           | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'}\|/\sigma)$$ | $$\sigma>0$$                  |
| Sigmoid           | $$\mathcal{K}(x_i, x_{i'})=\text{tanh}(\beta x_i^Tx_{i'} + \theta)$$ | $$\beta>0, \theta<0$$         |

How to choose?

1. If the number of features is large, and sample size is small, the data is usually linearly separable, then use linear kernel or logistic regression. 
2. If the number of features is small, and sample size is normal, then use RBF kernel.
3. If the number of features is small, and sample size is large, then add some features and it becomes the first case.

Note that SVMs with linear kernel are linear classifiers, and SVMs with other kernels are non-linear classifiers. 

## SVR

In SVR (support vector regression), we tolerate an error $$\epsilon$$ between $$w^Tx+b$$ and $$y$$, which means the error will not be considered if it is within $-\epsilon$ and $$\epsilon$$. Note that $$\epsilon$$ is a hyperparameter that we should set before training the model.

<div align="center"> <img src="../pictures/SVR_2.png" alt="SVR_2" style="zoom:100%;" /> </div>

## Tips

The local optimum of SVM must be the global optimum, since it is a convex optimization problem. 

SVM is good for small sample size, compared to other machine learning methods, since we only need a few support vectors to determine the hyperplanes. SVM is also suitable for non-linear and high dimensional problems.  

SVM is not very sensitive to imbalanced dataset problem. The loss is composed by margin and support vectors, not by all data, thus SVM works well if the support vectors are balanced, and the balanced original dataset is not that necessary. 



<br>

**References**: 

周志华. *机器学习*. 清华大学出版社, 2016. 

张皓. [从零推导支持向量机（SVM）](https://link.zhihu.com/?target=https%3A//github.com/HaoMood/File/raw/master/%25E4%25BB%258E%25E9%259B%25B6%25E6%259E%2584%25E5%25BB%25BA%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA%28SVM%29.pdf).
