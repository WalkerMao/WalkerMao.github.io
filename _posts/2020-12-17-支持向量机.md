---
layout: post
title:  "支持向量机"
date: 2020-03-16
categories: ML
comments: true
published: true
hidden: true
---

## 引入

对于二元分类问题，一个基本的思想是在样本空间中找到一个超平面，将样本分为两类。对于任意给定的$$w\in\mathbb{R}^p，b\in\mathbb{R}$$,空间$$\mathbb{R}^p$$中的超平面可以表示为
$$
\{x\in\mathbb{R}^p:w^Tx+b=0\}，
$$

我们将此超平面记作$$(w,b)$$。


假设$$y\in\{-1,1\}$$中，如果超平面可以分离样本，那么

$$
\begin{cases}
w^Tx_i+b\geq 1, &\text{if }y_i=1, \\
w^Tx_i+b\leq-1, &\text{if }y_i=-1.
\end{cases}
$$

将两个**超平面**表示为

$$
\begin{aligned} & H_1 = \{x \in \mathbb{R}^p : w^Tx + b = 1\}, \\ & H_2 = \{x \in \mathbb{R}^p : w^Tx + b = -1\}. \end{aligned}
$$

$$H_1$$和$$H_2$$是平行的，因为$$H_1\cap H_2=\empty$$。我们希望这两个超平面将样本分开，并且超平面之间的间隔最大化。

$$H_1$$和$$H_2$$之间的**间隔**为$$\frac{2}{\|w\|}$$。证明：

我们首先证明，对于任意$$x_1\in\mathbb{R}^p$$，样本空间中的点$$x_1$$与超平面$$(w,b)$$之间的距离为 $$\frac{| w^Tx_1+b |}{\| w \|}$$。

对于超平面$$(w,b)$$上的任意两点$$x_2,x_3$$，$$x_2-x_3$$是超平面$$(w,b)$$上的向量，则$$w^T(x_2-x_3)=0$$，这意味着向量$$w$$垂直于$$(w，b)$$。将$$x_0$$表示为$$x_1$$在$$(w,b)$$上的投影点，$$x_1$$与$$(w,b)$$之间的距离为$$\| x_1-x_0\|$$，则向量$$x_1-x_0$$与$$(w,b)$$垂直，那么我们可以将其表示为$$x_1-x_0=\| x_1-x_0\|\frac{w}{\|w\|}$$。因此距离$$\|x_1-x_0\| = \sqrt{(x_1 - x_0)^T(x_1-x_0)} = \sqrt{\|x_1-x_0\| \frac{w^T}{\|w\|} (x_1 - x_0)} = \sqrt{ \frac{\|x_1-x_0\|}{\|w\|} (w^Tx_1 - w^Tx_0)} = \sqrt{ \frac{\|x_1-x_0\|}{\|w\|} (w^Tx_1 + b)}$$ $$ \implies \|x_1-x_0\|=\frac{| w^Tx_1+b |}{\| w \|}$$。

现在我们假设$$x_1$$在超平面$$H_1$$上，那么$$x_1$$和$$H_2$$之间的距离是$$\frac{| w^Tx_1+b+1 |}{\| w \|}=\frac{2}{\| w \|}. \square $$

我们注意到，最大化$$\frac{2}{\| w \|}$$与最小化$$\frac{\|w\|}{2}$$等价。

那么，对于**线性可分**的数据，支持向量机最大化两个超平面$$H_1,H_2$$的间隔：
$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|}{2}, \\
& \text{ subject to } y_i(w^Tx_i+b) \geq 1, \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

样本点$$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) = 1\} $$位于超平面$$H_1,H_2$$上，称为**支持向量**。

## 软边缘支持向量机

实际数据通常不是线性可分的。设

$$
z_i=y_i(w^Tx_i+b)-1，
$$

那么错误分类的样本是$$\{(x_i，y_i)：z_i<0\}$$。

我们可以用损失函数来衡量误分类的严重程度。如0/1损失：

$$
\ell_{\text{0/1}}(z)=\mathbf{1}(z<0).
$$

然而，这种损失函数不是凸函数，也不是连续的，因此不适合数值优化。因此，我们可以使用一些凸的连续的损失函数，称为**替代损失**函数。以下是三种常用的替代损失函数：

$$
\begin{aligned}
&\text{铰链损失 Hinge loss:}&\ell_{\text{hinge}}(z)=\max(0，1-z);\\
&\text{指数损失 Expometal loss:}&\ell_{\text{exp}}(z)=e^{-z};\\
&\text{对数损失 Logistic loss:}&\ell_{\text{log}}(z)=\ln(1+e^{-z}).
\end{aligned}
$$

以下是这四种损失函数的图形：

<div align="center"> <img src="../pictures/surrogate-loss-functions.png" alt="surrogate-loss-functions.png" style="zoom:38%;" /> </div>

相较于其他两个替代损失函数，合叶损失对训练数据中的极端样本更鲁棒。本文以下内容我们使用合叶损失作为替代损失函数。

对每一个样本$$(x_i,y_i)$$，我们引入**松弛变量**$$\xi_i$$

$$
\xi_i = \ell_{\text{hinge}}(z_i+1) = \max\left(0, 1 - y_i(w^Tx_i+b)\right).
$$

<div align="center"> <img src="../pictures/svm-hinge-loss.png" alt="svm-hinge-loss" style="zoom:50%;" /> </div>

对于给定的超参数 $$C > 0$$, 软间隔支持向量机的优化问题可表示为

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|}{2} + C \sum_{i=1}^{n} \xi_i,\\
& \text{ subject to } 
\begin{cases} 
y_i(w^Tx_i+b) \geq 1-\xi_i,  \\ \xi_i\geq0, 
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

样本点$$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) \leq 1\} $$是支持向量。

设$$\hat{w},\hat{b}$$为以上优化问题的最优解，那么**分隔超平面**为

$$
\{x\in\mathbb{R}^p:\hat{w}^T x+\hat{b}=0\}
$$

分类决策函数为

$$
\hat{y}=\text{sign}(\hat{w}^Tx+\hat{b})。
$$

## 对偶问题

### 对偶优化问题

软间隔支持向量机的优化问题对应的**拉格朗日函数**为
$$
\mathcal{L}(w,b,\xi,\alpha,\mu) = \frac{\|w\|^2}{2} + C \sum_{i=1}^{n} \xi_i + \sum_{i=1}^n \alpha_i [1 - \xi_i - y_i(w^Tx_i + b)] - \sum_{i=1}^n\mu_i\xi_i,
$$
其中$$\alpha_i \geq 0, \mu_i \geq 0$$是拉格朗日乘子。

设
$$
\mathcal{D}(\alpha,\mu) = \min_{w,b,\xi} \mathcal{L}(w,b,\xi,\alpha,\mu).
$$
则软间隔支持向量机的优化问题对应的对偶优化问题为
$$
\max_{\alpha, \mu: \alpha_i \geq 0, \mu_i \geq 0} \mathcal{D}(\alpha,\mu) = \max_{\alpha, \mu: \alpha_i \geq 0, \mu_i \geq 0} \min_{w,b,\xi} \mathcal{L}(w,b,\xi,\alpha,\mu).
$$
将函数$$\mathcal{L}(w,b,\xi,\alpha,\mu)$$分别对$$w,b,\xi_i$$求导，然后设为$$0$$, 得到

$$
\begin{aligned}
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial w} = w - \sum_{i=1}^{n} \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^{n} \alpha_i y_i x_i, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial b} = - \sum_{i=1}^{n} \alpha_i y_i = 0 \implies \sum_{i=1}^{n} \alpha_i y_i = 0, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies \alpha_i + \mu_i = C.
\end{aligned}
$$

代入回函数$$\mathcal{L}(w,b,\xi,\alpha,\mu)$$，则**对偶优化问题**可表示为

$$
\begin{equation*}
\begin{aligned}
& \max_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'}, \\
& \text{ subject to } 
\begin{cases} 
\sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C ,
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

**KKT条件**为
$$
\begin{cases}
\alpha_i \geq 0, \\
\mu_i \geq 0, \\
y_i(w^Tx_i+b) - 1 + \xi_i \geq 0, \\
\alpha_i[y_i(w^Tx_i+b) - 1 + \xi_i] = 0, \\
\xi_i \geq 0, \\
\mu_i\xi_i = 0.
\end{cases}
$$

### SMO算法

我们用**SMO** (序列最小最优化 sequential minimal optimization) 算法来解该对偶优化问题。其基本思路是：如果所有变量的解满足KKT条件，那么这些变量就是最优化问题的解，因为KKT条件是该最优化问题的充分必要条件。

迭代以下两步直至收敛 {

1. 找到一个违反KKT条件最严重的$$α_i$$，然后找到使得$$w^Tx_{i'}+b$$与真实值$$y_{i'}$$相差最大的$$i'$$对应的$$α_{i'}$$，这里的$$w=\sum_{l=1}^n \alpha_{l} y_{l}x_{l}$$.
2. 维持其他的$$α_{i''}$$不变，在对偶优化问题的约束条件下求解二次规划问题：$$\max_{\alpha_i,\alpha_{i'}}\mathcal{D}(\alpha)$$.
   }

该对偶优化问题的解为
$$
\hat{\alpha} = (\hat{\alpha}_1, \hat{\alpha}_2, \cdots, \hat{\alpha}_n) = \arg\max_{\alpha} \mathcal{D}(\alpha),
$$

SMO算法的详细介绍可参见李航的《统计学习方法》7.4 序列最小优化算法。

### 原问题的解

在求得解$$\hat{\alpha}$$后, 可计算

$$
\hat{w} = \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}.
$$

下标在以下集合$$I^*$$中的样本点$$(x_i,y_i)$$是支持向量：

$$
I^* = \{ i^*: \hat{\alpha}_{i^*} \neq 0 \} = \{ i^*: 0 < \hat{\alpha}_{i^*} \leq C \}.
$$

然后我们可通过任一支持向量$$(x_{i^*},y_{i^*})$$来计算截距项$$\hat{b}$$： 

$$
\hat{b} = y_{i^*} - \hat{w}^Tx_{i^*} = y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x_{i^*}.
$$

在实践中，出于鲁棒性的考虑，我们通常采用所有（或部分）支持向量计算的$$\hat{b}$$的平均值:

$$
\hat{b} = \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \hat{w}^Tx_{i^*} \right) = \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x_{i^*} \right).
$$

在得到$$\hat{w}$$和$$\hat{b}$$后，分隔超平面为

$$
\left\{ x\in\mathbb{R}^p: \hat{w}^T{x} + \hat{b} = 0 \right\} = \left\{ x\in\mathbb{R}^p: \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x +  \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}^T x_{i^*} \right) = 0 \right\}.
$$

分类决策函数为 

$$
\hat{y} = \text{sign} \left( \hat{w}^Tx + \hat{b} \right).
$$

### 支持向量

对于任一样本$$(x_i,y_i)$$, 有以下五种可能的情形。  

If $$\alpha_i=0$$, then $$\mu_i=C，\xi_i=0$$. The sample (e.g. $$x_1$$ in the plot below) does not affect the estimation of the hyperplane $$\left\{ x\in\mathbb{R}^p: \hat{w}^T{x} + \hat{b} = 0 \right\}$$, and this sample is not a support vector. 

Otherwise, when $$0 < \alpha_i \leq C$$, the sample is a support vector. 

If $$0<\alpha_i<C$$, then $$\mu_i=C-\alpha_i，\xi_i=0$$. This sample (e.g. $$x_2$$ in the plot below) is correctly classified, and on the hyperplane $$H_1$$ if $$y_i=1$$ or $$H_2$$ if $$y_i=-1$$. 

When $$\alpha_i=C$$, it follows that $$\mu_i=0$$, and $$\frac{\xi_i}{\|w\|}>0$$ is the distance between the sample and the hyperplane $$H_1$$ if $$y_i=1$$ or $$H_2$$ if $$y_i=-1$$. 

If $$\alpha_i=C$$ and $$0<\xi_i<1$$, then $$\mu_i=0$$. This sample (e.g. $$x_3$$ in the plot below) is correctly classified. 

If $$\alpha_i=C$$ and $$\xi_i=1$$, then $$\mu_i=0$$. This sample (e.g. $$x_4$$ in the plot below) is on the separating hyperplane. 

If $$\alpha_i=C$$ and $$\xi_i>1$$, then $$\mu_i=0$$. This sample (e.g. $$x_5,x_6$$ in the plot below) is incorrectly classified and beyond the separating hyperplane. 

<img src="file:////private/var/folders/7v/vqpstkms38b9cs6kgxnln91h0000gn/T/com.kingsoft.wpsoffice.mac/wps-weikaimao/ksohtml/wpsJlwkfG.png" alt="img" style="zoom:80%;" />

<br>

这五种情形总结为表格

| $$\alpha_i$$     | $$\mu_i$$      | $$\xi_i$$     | 是否正确分类？ | 是否为支持向量？ | 是否在超平面上？                                             |
| ---------------- | -------------- | ------------- | -------------- | ---------------- | ------------------------------------------------------------ |
| $$0$$            | $$C$$          | $$0$$         | Yes            | No               | No                                                           |
| $$0<\alpha_i<C$$ | $$C-\alpha_i$$ | $$0$$         | Yes            | Yes              | On $$H_1$$ if $$y_i=1$$ or $$H_2$$ if $$y_i=-1$$.            |
| $$C$$            | $$0$$          | $$0<\xi_i<1$$ | Yes            | Yes              | No                                                           |
| $$C$$            | $$0$$          | $$1$$         | /              | Yes              | On the separating hyperplane.                                |
| $$C$$            | $$0$$          | $$\xi_i>1$$   | No             | Yes              | In case $$\xi_i=2$$, on $$H_2$$ if $$y_i=1$$ or $$H_1$$ if $$y_i=-1$$. Otherwise no. |

## 核SVM

### 映射到高维空间

If the data is not linearly separable and it has finite features, mapping to a higher dimensional feature space makes it linearly separable. 

As an example shown below, the left plot shows that the data is not linearly separable in original feature space, while the right plot shows that the data is linearly separable in a higher dimensional feature space.

<div align="center"> <img src="../pictures/Kernel-SVM.png" alt="Kernel-SVM" style="zoom:70%;" /> </div>

Denote $$\phi:\mathbb{R}^p \to \mathbb{R}^\tilde{p}$$ as a mapping function and $$\phi(x)$$ as the higher dimensional feature vector after mapping $$x$$ to a higher dimensional feature space:

$$
x \in \mathbb{R}^p \to \phi(x) \in \mathbb{R}^\tilde{p}.
$$

Then the two hyperplanes will be 

$$
\{x \in \mathbb{R}^p : w^T \phi(x) + b = \pm 1\}.
$$

The soft margin SVM is then: 

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|}{2} + C \sum_{i=1}^{n} \xi_i, \\
& \text{ subject to } 
\begin{cases} 
 y_i(w^T \phi(x_i)+b) \geq 1-\xi_i,  \\ 
 \xi_i \geq 0, 
\end{cases}
\ \ \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

The dual optimization problem:

$$
\begin{equation*}\begin{aligned}& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \phi(x_i)^T \phi(x_{i'}), \\
& \text{ subject to } \begin{cases} \sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C , \end{cases} \ \ \forall i=1,\cdots,n. \end{aligned} \end{equation*}
$$

### 核技巧

Since the feature vector $$\phi(x)$$ may have a very high dimension $$\tilde{p}$$, the computation of the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ is $$O(\tilde{p})$$ and is extremely expensive. To deal with that, we use the **kernel trick**. 

A **kernel function** is defined as

$$
\mathcal{K}(x_i, x_{i'}) =  \phi(x_i)^T \phi(x_{i'}).
$$

With the kernel function, we can compute the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ by using $$x_i, x_{i'}$$ that are in the **original space**, and the computation of $$\mathcal{K}(x_i, x_{i'})$$ is $$O(p)$$. By using kernel function, the computation is reduced from $$O(\tilde{p})$$ to $$O(p)$$. That means computing $$\mathcal{K}(x_i, x_{i'})$$ is much cheaper than computing $$ \phi(x_i)^T \phi(x_{i'}) $$ directly.

Applying kernel trick on SVM, the dual optimization problem can be rewritten as
$$
\begin{equation*}\begin{aligned}& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \mathcal{K}(x_i, x_{i'}), \\
& \text{ subject to } \begin{cases} \sum_{i=1}^n \alpha_iy_i=0,  \\ 
0\leq\alpha_i\leq C , \end{cases} \ \ \forall i=1,\cdots,n. \end{aligned} \end{equation*}
$$
The separating hyperplane is
$$
\left\{ x\in\mathbb{R}^p: \hat{w}^T{x} + \hat{b} = 0 \right\} = \left\{ x\in\mathbb{R}^p: \sum_{i=1}^n \hat{\alpha}_{i} y_{i}\mathcal{K}(x_i, x) +  \frac{1}{\mid I^* \mid} \sum_{i^* \in I^*} \left( y_{i^*} - \sum_{i=1}^n \hat{\alpha}_{i} y_{i}\mathcal{K}(x_i, x_{i^*}) \right) = 0 \right\}.
$$

The **kernel matrix** is defined as 

$$
\mathbf{K} = 
\begin{pmatrix}
  \mathcal{K}(x_1, x_1) & \mathcal{K}(x_1, x_2) & \cdots & \mathcal{K}(x_1, x_{n}) \\
\mathcal{K}(x_2, x_1) & \mathcal{K}(x_2, x_2) & \cdots & \mathcal{K}(x_2, x_{n}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \mathcal{K}(x_n, x_1) & \mathcal{K}(x_n, x_2) & \cdots & \mathcal{K}(x_n, x_{n})
 \end{pmatrix}.
$$

Kernel matrix is always positive semi-definite. Actually a function $$\mathcal{K}(x_i, x_{i'})$$ can be used as a kernel function if and only if it is symmetric and the corresponding kernel matrix is positive semi-definite. The function is symmetric means that $$\mathcal{K}(x_i, x_{i'}) = \mathcal{K}(x_{i'}, x_i)$$ for all $$i,i'$$.

Some commonly used kernel functions:

| Name              | Formula                                                      | Hyperparameter             |
| ----------------- | ------------------------------------------------------------ | -------------------------- |
| Linear            | $$\mathcal{K}(x_i, x_{i'})=x_i^Tx_{i'}$$                     |                            |
| Polynomial        | $$\mathcal{K}(x_i, x_{i'})=(x_i^Tx_{i'})^d$$                 | polynomal degree $$d > 1$$ |
| RBF (or Gaussian) | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'} \|/2\sigma^2)$$ | width $$\sigma>0$$         |
| Laplace           | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'}\|/\sigma)$$ | $$\sigma>0$$               |
| Sigmoid           | $$\mathcal{K}(x_i, x_{i'})=\text{tanh}(\beta x_i^Tx_{i'} + \theta)$$ | $$\beta>0, \theta<0$$      |

How to choose?

1. If the number of features is large, and sample size is small, the data is usually linearly separable, then use linear kernel or logistic regression. 
2. If the number of features is small, and sample size is normal, then use RBF kernel.
3. If the number of features is small, and sample size is large, then add some features and it becomes the first case.

Note that SVMs with linear kernel are linear classifiers, and SVMs with other kernels are non-linear classifiers. 

## 其他

支持向量机的优化问题是凸优化，因此其解一定是全剧最优解。

由于支持向量机的构建只依赖于支持向量，相较于其他算法, 支持向量机更适合小样本高维数据的的建模。

支持向量机的构建对样本不均衡问题不太敏感，因为其优化目标极小化超平面间隔和支持向量带来的松弛变量。所以只要支持向量相对均衡，支持向量机受样本不均衡问题的影响就不会太大。

<br>

**References**: 

周志华. 第6章 支持向量机. *机器学习*. 清华大学出版社, 2016. 

李航. 第7章 支持向量机. *统计学习方法*. 清华大学出版社, 2012. 

张皓. [从零推导支持向量机(SVM)](https://link.zhihu.com/?target=https%3A//github.com/HaoMood/File/raw/master/%25E4%25BB%258E%25E9%259B%25B6%25E6%259E%2584%25E5%25BB%25BA%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA%28SVM%29.pdf).
