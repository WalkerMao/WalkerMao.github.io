---
layout: post
title:  "Basic Neural Networks"
date: 2020-03-20
categories: DL
comments: true
---

## Representational Power

Kolmogorov Arnold Representation theorem states that any continuous function $f$ can be exactly represented by (possible infinite) sequence of addition, multiplication and composition with functions that are universal (do not depend on $f$). 

It can be shown (e.g. see [*Approximation by Superpositions of Sigmoidal Function*](http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf), or this [intuitive explanation](http://neuralnetworksanddeeplearning.com/chap4.html)) that given any continuous function $f(x)$ and some $ϵ>0$, there exists a Neural Network $\hat{f}(x)$ with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $ \forall x, \mid f(x)−\hat{f}(x) \mid <  \epsilon$. In other words, the one hidden layer neural network can approximate any continuous function.

Despite the fact that their representational power is theoretically equal, in practice, as we increase the size and number of layers in a Neural Network, the capacity of the network increases. That is, the space of representable functions grows since the neurons can collaborate to express many different functions. 

## Units

### Single Unit

Taking a weighted sum of its inputs and then applying a activation function. $\sigma(z)=\sigma(w^Tx+b)$, where the vector $w$ is called **weight**, the scalar $b$ is called **bias**, the function $\sigma(\cdot)$ is called activation function. 

For linear regression, $\sigma(z)=z$. For logistic regression, $\sigma(z)=\frac{1}{1+e^{-z}}$. 

<img src="/pictures/single-neuron.png" alt="1573539428065" style="zoom: 42%;" /><img src="/pictures/activation-functions.png" alt="activation functions" style="zoom: 78%;" />

### Activation Functions

Activation functions are usually non-linear. If they are all linear, then we simply get the linear combination of the inputs, and that is the same as linear regression. 

#### Sigmoid, Tanh

They both have S-shape curve. Note that $$\text{Tanh}(z) = 2\cdot\text{Sigmoid}(2z)  - 1$$.

$$
\text{Sigmoid: }\sigma(z)=\frac{1}{1+e^{-z}}; 
\text{ Tanh: } \sigma(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}.
$$

A major drawback of sigmoid and tanh is the **vanishing gradient** problem. When the neuron’s activation saturates at either tail of $$0$$ or $$1$$, the gradient at these regions is almost zero. During backpropagation, this (local) gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively "kill" the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. 

Another drawback of sigmoid is that its outputs are **not zero-centered**. This is not a problem of tanh, and that's why tanh is usually better than sigmoid. This is a drawback since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. It has less severe consequences compared to the saturated activation problem above. 

#### ReLU, Leaky ReLU, ELU

ReLU is the short for rectified linear units, and ELU is short for exponential linear unit. 

$$
\text{ ReLU: } \sigma(z)=\max(z,0); \\
\text{ Leaky ReLU: } \sigma(z)=\max(z, \alpha z)=
\begin{cases}
  \alpha z & \text{ for } z<0, \\ 
  z & \text{ for } z\geq 0;
\end{cases} \\
\text{ ELU: } \sigma(z)=\max(z, \alpha (e^z-1))=
\begin{cases}
  \alpha (e^z-1) & \text{ for } z<0, \\ 
  z & \text{ for } z\geq 0.
\end{cases}
$$

where $\alpha$ is a small positive number. e.g. $0.01$.

Pros of ReLU: 1. It greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is due to its linear, **non-saturating** form; 2. Computation is easier.

Cons of ReLU: 1. ReLU units can be **fragile** during training and can "die". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. However, with a lower learning rate this is less frequently an issue; 2. The range of ReLu is $$ [0, \infty ) $$. This means it can blow up the output of the activation function. 

Leaky ReLUs are one attempt to fix the "dying ReLU" problem. Instead of the function being zero when $x<0$, a leaky ReLU will instead have a small positive slope (of $0.01$, or so). However, it not always have benefits. 

Pros of ELU: ELU is smooth at the point $$0$$ and becomes smooth slowly until its output equals to $$-α$$, whereas ReLU or leaky ReLU is sharp at the point $$0$$. 

Cons of ELU: 1. 2. same as that of ReLU; 3. more computationally expensive since it involves exponential operations. 

| Pros                 | ReLU | Leaky ReLU | ELU  |
| -------------------- | ---- | ---------- | ---- |
| Non-saturating       | ✓    | ✓          | ✓    |
| Computationally easy | ✓    | ✓          | ✗    |
| Smooth               | ✗    | ✗          | ✓    |

| Cons                   | ReLU | Leaky ReLU | ELU  |
| ---------------------- | ---- | ---------- | ---- |
| Fragile (dead neurons) | ✓    | ✗          | ✗    |
| Blow up the output     | ✓    | ✓          | ✓    |

#### Maxout

$$
\text{ Maxout: } \sigma(z^{[1]},...,z^{[m]})=\max({w^{[1]}}^Tx + b^{[1]}, ...,{w^{[m]}}^Tx + b^{[m]}).
$$

Maxout neuron (introduced recently by [Goodfellow et al.](http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html)) generalizes the ReLU and its leaky version. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $\max(w^Tx+b, 0^Tx+0)$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, compared to ReLU, it have $m$ times of the number of parameters for every single neuron, leading to a high total number of parameters. 

*What neuron type should I use?* Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of "dead" units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.

## Feed-Forward Neural Networks

A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. The core of the neural network is the **hidden layer** formed of **hidden units**, each of which is a neural unit. In the standard architecture, each layer is **fully-connected**. 

<div style="text-align: center"> <img src="http://www.astroml.org/_images/fig_neural_network_1.png" alt="Feed-Forward Neural Networks" style="zoom:80%;" /> </div>

Let's assume there are $n_1$ hidden units in hidden layer $1$ and $n_2$ units in layer $2$. The output of the hidden layer $1$ can be expressed by $x^{[2]}=\sigma(W^{[1]} x^{[1]} + b^{[1]})$, where the outputs $x^{[2]} \in \mathbb{R}^{n_2}$, the weight matrix $W^{[1]} \in \mathbb{R}^{n_2 \times n_1}$, the inputs $x^{[1]} \in\mathbb{R}^{n_1}$, the bias vector $b\in\mathbb{R}^{n_1}$.

We apply **softmax** function at output layer for $K$-class classification: 

$$
\sigma (\mathbf {z} )_k={\frac {e^{z_k}}{\sum _{l=1}^{K}e^{z_{l}}}}{\text{ for }}k=1,\dotsc ,K {\text{, where }}\mathbf {z} =(z_{1},\dotsc ,z_{K})\in \mathbb {R} ^{K}.
$$

The output layer thus gives the **estimated probabilities** of each output node (corresponds to each class). If the number of classes is too large, it may be helpful to use [Hierarchical Softmax](http://arxiv.org/pdf/1310.4546.pdf).

We traditionally don't count the input layer when numbering layers, but do count the output layer.

## Optimization

The goal of the training procedure is to learn parameters $W^{[l]}$ and $b^{[l]}$ for each layer $l$ that minimize the loss function plus regularization term.

### Loss Function 

**Cross-entropy loss** function for $K$-class classification: $$L(y_i, \hat{p}_i) = -\sum_{k=1}^{K} I(y_i \text{ in class } k)\log(\hat{p}_{ik})$$, where $$\hat{p}_{ik}$$ is the estimated probability of $y_i$ belongs to class $k$. For binary classification, $L(y_i, \hat{p}_i) = -y_i\log(\hat{p}_i) - (1-y_i)\log(1-\hat{p}_i)$, where $y_i\in\{0,1\}$, $\hat{p}_i$ is the estimated probability of $y_i=1$. The estimated probabilities $\hat{p}_k$ by softmax function of the class $k$ is $\frac{e^{z_k}}{\sum _{k=1}^{K}e^{z_k}}$. 

### Initialization

If the weights in a network start too small/large, then the signal shrinks/grows as it passes through each layer until it is too tiny/massive to be useful, because it may lead to vanishing or exploding gradients. 

The appropriate initialization should have the following rules of thumb: 

1. The mean of the activations should be zero;
2. The variance of the activations should stay the same across every layer.

**Xavier** initialization: For every layer $l$: $W^{[l]} \sim N(0,1/n^{[l-1]}),\ b^{[l]}=0$. 

### Backpropagation

We use GD or SGD to optimize the objective function $\text{Obj}(w)$. For weight, we have $w_t=w_{t-1} - \eta \frac{\partial\ \text{Obj}(w_{t-1})}{\partial\ w_{t-1}}$, where $\eta$ is the step-size (learning rate).

The solution to computing the gradient is an algorithm called error **backpropagation**, which use the chain rules in calculus. (Note: the regularization term is not considered in the pictures below.) 

<div style="text-align: center"> <img src="/pictures/backpropagation.png" style="zoom: 50%;"/> </div> 

## Hyperparameters Tuning

The most common hyperparameters in Neural Networks include:

- the number of neurons (width)

- the number of layers (depth)

- the initial learning rate

- learning rate decay schedule (such as the decay constant)

- regularization strength (L2 penalty, dropout strength)

### Number of Neurons

Here are 3 rule-of-thumb methods for determining an acceptable number of neurons to use in the hidden layers [[Reference](https://www.heatonresearch.com/2017/06/01/hidden-layers.html)]. The number of hidden neurons should be: 

- between the size of the input layer and the size of the output layer.
- 2/3 the size of the input layer, plus the size of the output layer.
- less than twice the size of the input layer.

### Use a Single Validation Set

In practice, we prefer one validation fold to cross-validation. In most cases a single validation set of respectable size substantially simplifies the code base, without the need for cross-validation with multiple folds.

### Hyperparameter Ranges

Search for learning rate and regularization strength on log scale. For example, a typical sampling of the learning rate would look as follows: `learning_rate = 10 ** uniform(-6, 1)`. That is, we are generating a random number from a uniform distribution, but then raising it to the power of $$10$$. 

### Prefer Random Search

We prefer random search to grid search. As argued by Bergstra and Bengio in [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf), "randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid". As it turns out, this is also usually easier to implement. 

<div style="text-align: center"> <img src="/pictures/gridsearchbad.jpeg" alt="Grid Search and Random Search" style="zoom:50%;" /> </div>

## Regularization

We usually need to regularize the weight parameters, but It is not common to regularize the bias parameters because they do not interact with the data through multiplicative interactions, and therefore do not have the interpretation of controlling the influence of a data dimension on the final objective. 

### Why Don't Control Depth

Why we don't control the depth of networks to avoid overfitting?

In practice, it is always better to use the following methods to control overfitting instead of controlling the depth. In terms of optimization of objective, compared to shallow NNs, the deeper NNs contain significantly more local minima, but these minima turn out to be much better than that of shallow NNs in terms of their actual loss. Since Neural Networks are non-convex, it is hard to study these properties mathematically. 

Additionally, the final loss of a shallower network can have a higher variance in practice, it may due to the number of minimas is small. Sometimes we get a good minima, and sometimes we get a very bad one, and it relies on the random initialization. However, for a deep network, all solutions are about equally as good, and rely less on the luck of random initialization. 

In conclusion, you should use as big of a neural network as your computational budget allows, and use other regularization techniques to control overfitting. 

### L1 regularization

L1 regularization is another relatively common form of regularization, where for each weight $w$ we add the term $$λ \| w \|_1$$ to the objective.

The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e. very close to exactly zero). In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the "noisy" inputs. 

### L2 regularization

L2 regularization is perhaps the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. That is, for every weight $w$ in the network, we add the term $$\frac{1}{2}λ\|w\|_2^2$$ to the objective, where $λ$ is the regularization strength. (It is common to see the factor of $\frac{1}{2
}$ in front because then the gradient of this term with respect to the parameter $w$ is simply $λw$ instead of $2λw$.) 

The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. Lastly, notice that during gradient descent parameter update, using the L2 regularization ultimately means that every weight is decayed linearly: `W += -lambda * W` towards zero. 

In comparison, final weight vectors from L2 regularization are usually diffuse, small numbers. In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1. 

It is possible to combine the L1 regularization with the L2 regularization: $$λ_1 \| w \|_1 + λ_2 \| w \|_2^2$$ (this is called [Elastic net regularization](http://web.stanford.edu/~hastie/Papers/B67.2 (2005) 301-320 Zou & Hastie.pdf)). 

### Max Norm Constraints

Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector $w$ of every neuron to satisfy $$\|w\|_2<c$$. Typical values of $c$ are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot "explode" even when the learning rates are set too high because the updates are always bounded.  

### Dropout

Dropout is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability $$p$$ (a hyperparameter), or setting it to zero otherwise. 

<div style="text-align: center"> <img src="/pictures/dropout.jpeg" alt="Dropout" style="zoom:50%;" /> </div>

## Model Ensembles

One approach to improving the performance of neural networks is to train multiple independent models, and then average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). The improvements are more dramatic with higher model variety in the ensemble. 

There are a few approaches to forming an ensemble:

- **Same model, different initializations**. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.
- **Top models discovered during cross-validation**. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn’t require additional retraining of models after cross-validation. 
- **Different checkpoints of a single model**. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.
- **Running average of parameters during training**. We average the states of the network over last several iterations. You will find that this "smoothed" version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode.

One disadvantage of model ensembles is that they take longer to evaluate on test example.  

## Tips

### Regression and Classification

As for neural networks, the L2 loss (MSE) for regression problem is much harder to optimize than a more stable loss such as Softmax for classification problem. Intuitively, it requires a very specific property from the network to output exactly one correct value for each input (and its augmentations). The **L2 loss is less robust** because outliers can introduce huge gradients. 

When faced with a regression task, first consider if it is absolutely necessary. Instead, have a strong preference to discretizing your outputs to bins and perform classification over them whenever possible. For example, if you are predicting star rating for a product, it might work much better to use 5 independent classifiers for ratings of 1-5 stars instead of a regression loss.

If you’re certain that classification is not appropriate, use the L2 but be careful: For example, the L2 is more fragile and applying dropout in the network (especially in the layer right before the L2 loss) is not a great idea. 

​	

**References**:

Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. [*Deep learning*](http://www.deeplearningbook.org/). MIT press, 2016.

[CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/).