---
layout: post
title:  "Linear and Logistic Regression"
date: 2020-03-08
categories: ML
comments: true
---

## 1. Linear Regression

A linear regression model assumes that the regression function $$E(y_i \mid x_i)$$ is linear in the inputs $$ x_i^{(1)}, \cdots, x_i^{(p)} $$. The linear regression model has the form $$ \hat{y}_i = f(x_i) =  \beta_0 + \underset{1 \times p}{\beta^T} \ \underset{p \times 1}{x_i} = \beta_0 + \sum_{j=1}^{p} \beta^{(j)} x_i^{(j)} $$. If we set a $$1$$ in the first position of the vector $$x_i$$, then we can dismiss the intercept and the regression model can be written as $$ \hat{y}_i = f(x_i) = \underset{1 \times p}{\beta^T} \ \underset{p \times 1}{x_i} = \sum_{j=1}^{p} \beta^{(j)} x_i^{(j)} $$.

​	

The residual sum of squares: $$ \text{RSS}(\beta) = (y-X \beta)^T (y-X \beta) = \|y - X\beta \|^2_2 = \sum_{i=1}^{n}(y_i - \beta^T x_i)^2$$.

OLS estimator $$\hat{\beta} = \underset{\beta \in \mathbb{R}^p}{\text{argmin}} \ \text{RSS}(\beta) = (X^T X)^{-1} X^T y$$. 

Hat matrix $$H = X (X^T X)^{-1} X^T$$, which is positive semi-definite.

Assume $$\epsilon \sim N(0, \sigma^2)$$, then $$\hat{\beta} \sim N(\beta, (X^TX)^{-1} \sigma^2)$$. 

More on *The elements of Statistical Learning* Page 47-49. 

​	

Derivation of OLS estimator: The first and second partial derivatives are $$\frac{\partial \text{RSS}(\beta)}{\partial \beta} = -2 X^T (y-X \beta), \frac{\partial^2 \text{RSS}(\beta)}{\partial \beta \partial \beta^T} = 2 X^T X$$. Assuming that $$X$$ has full column rank, and hence $$X^TX$$ is positive definite, we set the first derivative to zero and then we get the solution $$\hat{\beta} = (X^T X)^{-1} X^T y$$.

​	

**Ridge regression**: $$\hat{\beta} = \underset{\beta \in \mathbb{R}^p}{\text{argmin}} \Big(\| y - X\beta \| ^2_2 + \lambda \| \beta \|^2_2 \Big) = (X^T X + \lambda I)^{-1} X^T y$$.

**LASSO**: $$ \hat{\beta} = \underset{\beta \in \mathbb{R}^p}{\text{argmin}} \Big( \|Y - X\beta \|^2_2 + \lambda \| \beta \|_1 \Big) $$.

​	

## 2. Logistic Regression (LR)

Logistic regression is used for classification problem and it can predict the probability of each class.

### 2.1 Multi-class LR

We use the softmax function to generate the probability. The logistic regression for $K$-class classification can be written as

$$
\hat{p}_{ik} = \frac{e^{z_{ik}}}{\sum_{k=1}^K e^{z_{ik}}}, \\
z_{ik} = w_k^T x_{i} + b_k,
$$

where $k$ is the index of class, the weights (coefficients) $w_k$ and and the $i$-th observation $x_i$ are both the vectors of dimension $p\times 1$; the bias $b_k$ is a scalar; $\hat{p}_{ik}$ and $z_{ik}$ are both scalar,  and $$\hat{p}_{ik}$$ is the estimated probability of the $i$-th observation is of class $k$.

Cross entropy loss for $K$-class classification problem is 

$$
\frac{1}{n} \sum_{i=1}^n \text{Loss}(y_i,\hat{p}_i) = \frac{1}{n} \sum_{i=1}^n \sum_{k=1}^K -y_{ik} \log \hat{p}_{ik}, 
$$

In the formula above, $y_i$ and $\hat{p}_ i$ are the vectors with dimension $K \times 1$; $y_{ik}$ and $$\hat{p}_{ik}$$ are the $k$-th elements in the vectors $y_i$ and $$\hat{p}_i$$ respectively; $y_{ik}=1$ if the $i$-th observation is of class $k$, and $y_{ik}=0$ otherwise. 

### 2.2 Binary LR

For binary classification, the softmax function is degraded to sigmoid function. Then the binary logistic regression can be written as

$$
\hat{p}_i = \frac{1}{1+e^{-z_i}}, \\z_i = w^Tx_i + b,
$$

where the weights (coefficients) $w$ and and the $i$-th observation $x_i$ are both the vectors of dimension $p\times 1$; the bias $$b$$ is a scalar; $\hat{p}_i$ and $z_i$ are both scalar; $$\hat{p}_{i}$$ is the estimated probability of the $i$-th observation is of the positive class.

For binary classification problem, the cross entropy is 

$$
\frac{1}{n} \sum_{i=1}^n \big[ -y_i \log \hat{p}_i - (1-y_i) \log(1-\hat{p}_i) \big],
$$

where $y_i$ and $\hat{p}_i$ are both scalar, and $y_i$ can only be $0$ or $1$. 

#### 2.3 Gradient Descent for LR

For simplicity, let's look at the gradient descent for binary logistic regression.

The partial derivative of the loss with respect to $\hat{p}_i$:

$$
\frac{\partial \text{Loss}(y_i, \hat{p}_i)} {\partial \hat{p}_i} =  -\frac{y_i}{\hat{p}_i} + \frac{1-y_i}{1-p_i}.
$$

The partial derivative of $\hat{p}_i$ with respect to $z_i$:

$$
\frac{\partial \hat{p}_i} {\partial z_i} =  \frac{-e^{-z_i}}{(1+e^{-z_i})^2} = \frac{-1}{1+e^{-z_i}} \cdot \frac{e^{-z_i}}{1+e^{-z_i}} = -\hat{p}_i (1-\hat{p}_i).
$$

The partial derivative of $z_i$ with respect to $w$ and $$b$$:

$$
\frac{\partial z_i} {\partial w} = x_i, \frac{\partial z_i} {\partial b} = 1.
$$

Thus, we have the gradients:

$$
\frac{\partial \text{Loss}(y_i, \hat{p}_i)} {\partial w} =  \frac{\partial \text{Loss}(y_i, \hat{p}_i)} {\partial \hat{p}_i} \cdot \frac{\partial \hat{p}_i} {\partial z_i} \cdot \frac{\partial z_i} {\partial w} = (\hat{p}_i - y_i) x_i, \\
\frac{\partial \text{Loss}(y_i, \hat{p}_i)} {\partial b} = \frac{\partial \text{Loss}(y_i, \hat{p}_i)} {\partial \hat{p}_i} \cdot \frac{\partial \hat{p}_i} {\partial z_i} \cdot \frac{\partial z_i} {\partial b} = \hat{p}_i - y_i.
$$

The gradient descent of parameters can be shown as:

$$
w^{[t]} = w^{[t-1]} - \alpha \cdot \frac{1}{n}\sum_{i=1}^{n} \frac{\partial \text{Loss}(y_i, \hat{p}_i^{[t-1]})} {\partial w^{[t-1]}} = w^{[t-1]} - \alpha \cdot \frac{1}{n}\sum_{i=1}^{n} (\hat{p}_i^{[t-1]} - y_i) x_i, \\
b^{[t]} = b^{[t-1]} - \alpha \cdot \frac{1}{n}\sum_{i=1}^{n} \frac{\partial \text{Loss}(y_i, \hat{p}_i^{[t-1]})} {\partial b^{[t-1]}} = b^{[t-1]} - \alpha \cdot \frac{1}{n}\sum_{i=1}^{n} (\hat{p}_i^{[t-1]} - y_i).
$$

where $t$ is the index of iteration, and $\alpha$ is the learning rate (step size).