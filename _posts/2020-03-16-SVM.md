---
layout: post
title:  "Support Vector Machines"
date: 2020-03-16
categories: ML
comments: true
---

## Intro to SVM

Suppose $$y_i \in \{-1,1\}$$, denote the two hyperplanes as $$H_1 = \{x \in \mathbb{R}^p : w^Tx + b = 1\}$$ and $$H_2 = \{x \in \mathbb{R}^p : w^Tx + b = -1\}$$ for some $$w \in \mathbb{R}^p, b \in \mathbb{R}$$. 

The margin between $$H_1$$ and $$H_2$$ is $$\frac{2}{\| w \|_2}$$. Note that maximizing the margin $$\frac{2}{\| w \|_2}$$ is the same as minimizing $$\frac{\|w\|_2}{2}$$.

For linearly separable data, SVM (support vector machine) finds maximum-marginal linear separator:

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} \\
& \text{ subject to } y_i(w^Tx_i+b) \geq 1, \forall i=1,\cdots,n.
\end{aligned}
\end{equation*}
$$

The points $$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) = 1\} $$ are the support vectors.

Real data is usually not linearly separable. We add what we call slack variables $$\xi_i$$ to account for violations for linear separability. For a hyperparameter $$C > 0$$, the **soft margin SVM** is then:

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i\\
& \text{ subject to } 
\begin{aligned} 
& y_i(w^Tx_i+b) \geq 1-\xi_i,  \\ & \xi_i\geq0,\forall i=1,\cdots,n. \end{aligned}
\end{aligned}
\end{equation*}
$$

The points $$\{(x_i, y_i): y_i(\hat{w}^Tx_i+\hat{b}) \leq 1\} $$ are the support vectors.

Let $$\hat{w}, \hat{b}$$ be the solution of the optimization, then the **classifier** is $$\hat{f}(x) = \text{sign}(\hat{w}^Tx + \hat{b})$$.

​	

<img src="/pictures/svm_slack.png" alt="svm_slack" style="zoom:100%;" />

​	

## Lagrange Duality

The Lagrange function of the optimization problem of soft margin SVM is 

$$
\mathcal{L}(w,b,\xi,\alpha,\mu) = \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i + \sum_{i=1}^n \alpha_i [1 - \xi_i - y_i(w^Tx_i + b)] - \sum_{i=1}^n\mu_i\xi_i,
$$

where $$\alpha_i \geq 0, \mu_i \geq 0$$ are the Lagrange multipliers.

Take derivative of the Lagrange function with respect to $$w,b,\xi_i$$ and set equal to $$0$$, and we have

$$
\begin{aligned}
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial w} = w - \sum_{i=1}^{n} \alpha_i y_i x_i = 0 \implies w = \sum_{i=1}^{n} \alpha_i y_i x_i, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial b} = - \sum_{i=1}^{n} \alpha_i y_i = 0 \implies \sum_{i=1}^{n} \alpha_i y_i = 0, \\
& \frac{\partial \mathcal{L}(w,b,\xi,\alpha,\mu)}{\partial \xi_i} = C - \alpha_i - \mu_i = 0 \implies C = \alpha_i + \mu_i.
\end{aligned}
$$

Plug these three equations into the optimization formulation of soft margin SVM, then we get the dual optimization problem:

$$
\begin{equation*}
\begin{aligned}
& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'}x_i^Tx_{i'} \\
& \text{ subject to } 
\begin{aligned} 
& \sum_{i=1}^n \alpha_iy_i=0,  \\ 
& 0\leq\alpha_i\leq C ,\forall i=1,\cdots,n. 
\end{aligned}
\end{aligned}
\end{equation*}
$$

Note that there are inequity constraints in the optimization formulation, which produce the KKT condtions:

$$
\begin{cases}
\alpha_i \geq 0, \\
\mu_i \geq 0, \\
\alpha_i[y_i(w^Tx_i+b) - 1 + \xi_i] = 0, \\
\xi_i \geq 0, \\
\mu_i\xi_i = 0.
\end{cases}
$$

We use the **SMO** (sequential minimal optimization) algorithm to solve the dual optimization problem. It simply does the following:

Repeat till convergence {

1. Select some pair $$α_i$$ and $$α_{i'}$$ to update next (using a heuristic that tries to pick the two that will allow us to make the biggest progress towards the global maximum).

2. Reoptimize $$\mathcal{D}(\alpha)$$ with respect to $$α_i$$ and $$α_{i'}$$ , while holding all the other $$α_{i''}$$'s $$(i'' \neq i, i')$$ fixed.
   }

After getting the solution of the dual optimization problem $$\hat{\alpha}_i, i=1,\cdots,n$$, we have the weight

$$
\hat{w} = \sum_{i=1}^n \hat{\alpha}_{i} y_{i}x_{i}.
$$

The support vectors are the points $$(x_i,y_i)$$ that have the indexes in the set:

$$
\mathbb{I}^* = \{ i^*: 0<\hat{\alpha}_{i^*}<C \}.
$$

Then we compute the intercept $$\hat{b}$$ by the support vectors: 

$$
\hat{b} = \frac{1}{\mid\mathbb{I^*}\mid} \sum_{i \in \mathbb{I^*}} ( y_{i^*} - \hat{w}^Tx_{i^*} )
$$

The classifier is $$\hat{f}(x) = \text{sign}(\hat{w}^Tx + \hat{b})$$. 

​	

## Kernel SVM

If the data is not linearly separable and it has finite features, mapping to a higher dimensional feature space makes it linearly separable. 

<div style="text-align: center"> <img src="/pictures/Kernel-SVM.png" alt="Kernel-SVM" style="zoom: 60%;" /> </div>

Denote $$\phi(x)$$ as the higher dimensional feature vector after mapping $$x$$ to a higher dimensional feature space. Then the hyperplanes will be $$\{x \in \mathbb{R}^p : w^T \phi(x) + b = \pm 1\}$$. 

The soft margin SVM is then: 

$$
\begin{equation*}
\begin{aligned}
& \min_{w,b} \frac{\|w\|_2}{2} + C \sum_{i=1}^{n} \xi_i\\
& \text{ subject to } 
\begin{aligned} 
& y_i(w^T \phi(x_i)+b) \geq 1-\xi_i,  \\ & \xi_i\geq0,\forall i=1,\cdots,n. \end{aligned}
\end{aligned}
\end{equation*}
$$

The dual optimization problem:

$$
\begin{equation*}\begin{aligned}& \min_{\alpha} \mathcal{D}(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{i'=1}^n \alpha_i\alpha_{i'}y_iy_{i'} \phi(x_i)^T \phi(x_{i'}) \\
& \text{ subject to } \begin{aligned} & \sum_{i=1}^n \alpha_iy_i=0,  \\ 
& 0\leq\alpha_i\leq C ,\forall i=1,\cdots,n. \end{aligned}\end{aligned}\end{equation*}
$$

Since the feature vector $$\phi(x)$$ may have high dimension, the computation of the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ can be expensive. To deal with that, we define a kernel function 

$$
\mathcal{K}(x_i, x_{i'}) =  \phi(x_i)^T \phi(x_{i'}).
$$

By using kernel function, we can compute the inner product $$ \phi(x_i)^T \phi(x_{i'}) $$ by using $$x_i, x_{i'}$$ that are in the original space, and that is much computationally cheaper than computing $$ \phi(x_i)^T \phi(x_{i'}) $$ directly.

The **kernel matrix** is defined as 

$$
\mathbf{K} = 
\begin{pmatrix}
  \mathcal{K}(x_1, x_1) & \mathcal{K}(x_1, x_2) & \cdots & \mathcal{K}(x_1, x_{n}) \\
\mathcal{K}(x_2, x_1) & \mathcal{K}(x_2, x_2) & \cdots & \mathcal{K}(x_2, x_{n}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \mathcal{K}(x_n, x_1) & \mathcal{K}(x_n, x_2) & \cdots & \mathcal{K}(x_n, x_{n})
 \end{pmatrix}.
$$

Some commonly used kernel functions:

| Name              | Formula                                                      | Hyperparameter                |
| ----------------- | ------------------------------------------------------------ | ----------------------------- |
| Linear            | $$\mathcal{K}(x_i, x_{i'})=x_i^Tx_{i'}$$                     |                               |
| Polynomial        | $$\mathcal{K}(x_i, x_{i'})=(x_i^Tx_{i'})^d$$                 | polynomal degree $$d \geq 1$$ |
| RBF (or Gaussian) | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'} \|/2\sigma^2)$$ | width $$\sigma>0$$            |
| Laplace           | $$\mathcal{K}(x_i, x_{i'})=\exp (-\| x_i - x_{i'}\|/\sigma)$$ | $$\sigma>0$$                  |
| Sigmoid           | $$\mathcal{K}(x_i, x_{i'})=\text{tanh}(\beta x_i^Tx_{i'} + \theta)$$ | $$\beta>0, \theta<0$$         |

How to choose?

1. If the number of features is large, and sample size is small, the data is usually linearly separable, then use linear kernel or logistic regression. 
2. If the number of features is small, and sample size is normal, then use RBF kernel.
3. If If the number of features is small, and sample size is large, then add some features and it becomes the first case.

​	

## SVR

In SVR (support vector regression), we tolerate an error $$\epsilon$$ between $$w^Tx+b$$ and $$y$$, which means the error will not be considered if it is within $-\epsilon$ and $$\epsilon$$. Note that $$\epsilon$$ is a hyperparameter that we should set before training the model.

<img src="/pictures/SVR_2.png" alt="SVR_2" style="zoom:100%;" />

​	

**Reference**: 周志华. *机器学习*. 清华大学出版社, 2016. 